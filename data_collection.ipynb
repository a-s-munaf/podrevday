{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get All Tweets with #PodRevDay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading tweets...\n",
      "Saved 1703\n",
      "Done. Output file generated \"data/jan-sep-2020.csv\".\n"
     ]
    }
   ],
   "source": [
    "#!mkdir data\n",
    "!GetOldTweets3 --querysearch \"podrevday\" --since 2020-01-01 --until 2020-09-12 --output \"data/jan-sep-2020.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from geotext import GeoText\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import twint\n",
    "\n",
    "df_tweets = pd.read_csv('data/jan-sep-2020.csv', parse_dates=['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get User Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pod_rev_users = list(set(df_tweets.username))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SpiveySpecial'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pod_rev_users[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twitter_scraper import Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'html' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-9a51ca3b1215>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtwitter_scraper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mProfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprofile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mProfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bugraisguzar'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/twitter_scraper/modules/profile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, username)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mpage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"https://twitter.com/{username}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musername\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0musername\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__parse_profile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__parse_profile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/twitter_scraper/modules/profile.py\u001b[0m in \u001b[0;36m__parse_profile\u001b[0;34m(self, page)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_verified\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhtml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".ProfileHeaderCard-locationText\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'html' referenced before assignment"
     ]
    }
   ],
   "source": [
    "from twitter_scraper import Profile\n",
    "profile = Profile('bugraisguzar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'html' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-aaf372660b8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprofile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mProfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SpiveySpecial\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprofile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#profile.name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#profile.username\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/twitter_scraper/modules/profile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, username)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mpage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"https://twitter.com/{username}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musername\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0musername\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__parse_profile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__parse_profile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/twitter_scraper/modules/profile.py\u001b[0m in \u001b[0;36m__parse_profile\u001b[0;34m(self, page)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_verified\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhtml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".ProfileHeaderCard-locationText\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'html' referenced before assignment"
     ]
    }
   ],
   "source": [
    "profile = Profile(\"SpiveySpecial\")\n",
    "profile.location\n",
    "#profile.name\n",
    "#profile.username\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CRITICAL:root:twint.get:User:'NoneType' object is not subscriptable\n"
     ]
    }
   ],
   "source": [
    "c = twint.Config()\n",
    "c.Store_object = True\n",
    "c.Pandas = True\n",
    "\n",
    "#for user in pod_rev_users: \n",
    "c.Username = pod_rev_users[1]\n",
    "twint.run.Lookup(c)\n",
    "#Users_df = twint.storage.panda.User_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pod_rev_users = list(set(df_tweets.username))\n",
    "\n",
    "c = twint.Config()\n",
    "c.Store_object = True\n",
    "c.Pandas = True\n",
    "\n",
    "for user in pod_rev_users: \n",
    "    c.Username = user\n",
    "    twint.run.Lookup(c)\n",
    "Users_df = twint.storage.panda.User_df()\n",
    "\n",
    "users_df = Users_df.drop_duplicates()\n",
    "users_df.to_csv('data/user_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean User Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users = pd.read_csv('data/user_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users = df_users.loc[:, ['id','username', 'name', 'location', 'join_date', 'followers', 'following', 'likes', 'url', 'verified' ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def location_extraction (df):\n",
    "    '''Creates creates a geotext column to extract city and country info if possible'''\n",
    "\n",
    "    df.loc[:, \"location\"] = df.loc[:, \"location\"].fillna(\"blank\")\n",
    "    df.loc[:, \"geotext\"] = df.loc[:, \"location\"].apply(GeoText)\n",
    "    df.loc[:, 'city'] = df.loc[:, 'geotext'].apply(lambda x: x.cities)\n",
    "    df.loc[:, 'country'] = df.loc[:, 'geotext'].apply(lambda x: x.countries)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_users = location_extraction(df_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geonamescache import GeonamesCache\n",
    "gc = GeonamesCache()\n",
    "countries = gc.get_countries()\n",
    "country_info = pd.DataFrame(countries).T\n",
    "country_info = country_info.set_index('geonameid').reset_index()\n",
    "name_code = country_info.loc[:, [\"name\", \"iso3\"]]\n",
    "\n",
    "us_states = [\"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DC\", \"DE\", \"FL\", \"GA\", \n",
    "             \"HI\", \"ID\", \"IL\", \"IN\", \"IA\", \"KS\", \"KY\", \"LA\", \"ME\", \"MD\", \"MA\",\n",
    "             \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\", \"NM\", \"NY\",\n",
    "             \"NC\", \"ND\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\", \"SD\", \"TN\", \"TX\",\n",
    "             \"UT\", \"VT\", \"VA\", \"WA\", \"WV\", \"WI\", \"WY\", \"USA\", \"United States\",\n",
    "             'Seattle', \"Los Angeles\", \"Houston\", \"Atlanta\", \"Pittsburgh\"]\n",
    "\n",
    "us_state_names = [\"Alaska\", \"Alabama\", \"Arkansas\", \"American Samoa\", \"Arizona\", \"California\", \"Colorado\", \"Connecticut\", \n",
    "               \"District \", \"of Columbia\", \"Delaware\", \"Florida\", \"Georgia\", \"Guam\", \"Hawaii\", \"Iowa\", \"Idaho\", \"Illinois\", \n",
    "               \"Indiana\", \"Kansas\", \"Kentucky\", \"Louisiana\", \"Massachusetts\", \"Maryland\", \"Maine\", \"Michigan\", \"Minnesota\", \n",
    "               \"Missouri\", \"Mississippi\", \"Montana\", \"North Carolina\", \"North Dakota\", \"Nebraska\", \"New Hampshire\", \"New Jersey\", \n",
    "               \"New Mexico\", \"Nevada\", \"New York\", \"Ohio\", \"Oklahoma\", \"Oregon\", \"Pennsylvania\", \"Puerto Rico\", \"Rhode Island\", \n",
    "               \"South Carolina\", \"South Dakota\", \"Tennessee\", \"Texas\", \"Utah\", \"Virginia\", \"Virgin Islands\", \"Vermont\", \"Washington\", \n",
    "               \"Wisconsin\", \"West Virginia\", \"Wyoming\"]\n",
    "\n",
    "can_prov_abbrev = {'Alberta': 'AB','British Columbia': 'BC','Manitoba': 'MB', 'New Brunswick': 'NB',\n",
    "                       'Newfoundland and Labrador': 'NL', 'Northwest Territories': 'NT','Nova Scotia': 'NS','Nunavut': 'NU',\n",
    "                       'Ontario': 'ON','Prince Edward Island': 'PE', 'Quebec': 'QC','Saskatchewan': 'SK','Yukon': 'YT'}\n",
    "\n",
    "can_prov_names, can_prov_abbr = zip(*can_prov_abbrev.items())\n",
    "\n",
    "uk = [\"England\", 'Wales', \"Scotland\", 'London', \"Manchester\", \"Isle of Wight\", \"Northern Ireland\", \"United Kingdom\", 'Bailiwick of Guernsey', \"UK\", \"Hoxton\", \"Jersey\"]\n",
    "\n",
    "india_city = [\"Bangalore\", \"Delhi\", \"Hyderabad\", \"Bengaluru\"]\n",
    "\n",
    "german_city = [\"Munich\", \"Berlin\", \"eisgau\",\"Hamburg\", \"Dortmund\"]\n",
    "\n",
    "south_africa = [\"South Africa\", \"Durban\", \"Johannesburg\"]\n",
    "\n",
    "uae = ['UAE', 'Dubai', 'Abu Dhabi']\n",
    "\n",
    "def replacer(area, name):\n",
    "    df_users.loc[(df_users.location.str.contains('|'.join(area))), \"country\"] = name    \n",
    "    return df_users\n",
    "\n",
    "def list_to_string(df):\n",
    "    df[\"city\"] = df['city'].apply(lambda x: \"\".join(map(str, x)))\n",
    "    df[\"country\"] = df['country'].apply(lambda x: \"\".join(map(str, x)))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def location_cleaner(df):\n",
    "    df = replacer(can_prov_names, \"Canada\")\n",
    "    df = replacer(can_prov_abbrev, \"Canada\")\n",
    "    df = replacer(us_state_names, \"United States\")\n",
    "    df = replacer(us_states, \"United States\")\n",
    "    df = replacer(uk, \"United Kingdom\")\n",
    "    df = replacer(german_city, \"Germany\")\n",
    "    df = replacer(south_africa, \"South Africa\")\n",
    "    df = replacer(india_city, \"India\")\n",
    "    df = replacer(uae, 'United Arab Emirates')\n",
    "    df.loc[(df.location == \"Italia\"), \"country\"] = \"Italy\" \n",
    "    df.loc[(df.location == \"Belgrade\"), \"country\"] = \"Serbia\" \n",
    "    df.loc[(df.country == \"PolandSerbia\"), \"country\"] = \"Poland\" \n",
    "    df = list_to_string(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users = location_cleaner(df_users)\n",
    "df_users.loc[(df_users.country == \"PolandSerbia\"), \"country\"] = \"Poland\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                        162\n",
       "United States           101\n",
       "United Kingdom           30\n",
       "Canada                    7\n",
       "Germany                   5\n",
       "New Zealand               3\n",
       "India                     3\n",
       "Australia                 3\n",
       "South Africa              3\n",
       "Malaysia                  2\n",
       "Poland                    2\n",
       "France                    2\n",
       "United Arab Emirates      1\n",
       "Serbia                    1\n",
       "Sweden                    1\n",
       "Nigeria                   1\n",
       "Ecuador                   1\n",
       "Italy                     1\n",
       "Name: country, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_users.country.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users_full = pd.merge(left=df_users,\n",
    "                    right=name_code,\n",
    "                    how='left',\n",
    "                    left_on='country',\n",
    "                    right_on='name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users_full = df_users_full.drop('name_y',axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>username</th>\n",
       "      <th>name_x</th>\n",
       "      <th>location</th>\n",
       "      <th>join_date</th>\n",
       "      <th>followers</th>\n",
       "      <th>following</th>\n",
       "      <th>likes</th>\n",
       "      <th>url</th>\n",
       "      <th>verified</th>\n",
       "      <th>geotext</th>\n",
       "      <th>city</th>\n",
       "      <th>country</th>\n",
       "      <th>iso3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1228314814021357568</td>\n",
       "      <td>BackTrackerUK</td>\n",
       "      <td>The BackTracker History Show</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>14 Feb 2020</td>\n",
       "      <td>418</td>\n",
       "      <td>685</td>\n",
       "      <td>1089</td>\n",
       "      <td>https://www.bradleystokeradio.com/</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;geotext.geotext.GeoText object at 0x7f43b453a...</td>\n",
       "      <td></td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>GBR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>944815878</td>\n",
       "      <td>myostaff</td>\n",
       "      <td>Berny</td>\n",
       "      <td>Sydney, Australia</td>\n",
       "      <td>12 Nov 2012</td>\n",
       "      <td>202</td>\n",
       "      <td>184</td>\n",
       "      <td>264</td>\n",
       "      <td>https://www.knowledgebroker.online</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;geotext.geotext.GeoText object at 0x7f43b453a...</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>Australia</td>\n",
       "      <td>AUS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>433185027</td>\n",
       "      <td>Annelinda_c</td>\n",
       "      <td>Anne with an E</td>\n",
       "      <td>South Africa</td>\n",
       "      <td>9 Dec 2011</td>\n",
       "      <td>880</td>\n",
       "      <td>838</td>\n",
       "      <td>3881</td>\n",
       "      <td>https://linktr.ee/RootofSciencePodcasts</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;geotext.geotext.GeoText object at 0x7f43b453a...</td>\n",
       "      <td></td>\n",
       "      <td>South Africa</td>\n",
       "      <td>ZAF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1101617456945287169</td>\n",
       "      <td>StarWarsSession</td>\n",
       "      <td>Star Wars Sessions Podcast</td>\n",
       "      <td>Essex, UK</td>\n",
       "      <td>1 Mar 2019</td>\n",
       "      <td>1288</td>\n",
       "      <td>1607</td>\n",
       "      <td>16529</td>\n",
       "      <td>http://patreon.com/starwarssessions</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;geotext.geotext.GeoText object at 0x7f43b453a...</td>\n",
       "      <td>Essex</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>GBR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>821846</td>\n",
       "      <td>kelake</td>\n",
       "      <td>Clark MacLeod （克拉克）</td>\n",
       "      <td>Stratford, Prince Edward Island</td>\n",
       "      <td>8 Mar 2007</td>\n",
       "      <td>674</td>\n",
       "      <td>558</td>\n",
       "      <td>7373</td>\n",
       "      <td>http://clarkmacleod.com</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;geotext.geotext.GeoText object at 0x7f43b453a...</td>\n",
       "      <td>StratfordPrince Edward</td>\n",
       "      <td>Canada</td>\n",
       "      <td>CAN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id         username                        name_x  \\\n",
       "0  1228314814021357568    BackTrackerUK  The BackTracker History Show   \n",
       "1            944815878         myostaff                         Berny   \n",
       "2            433185027      Annelinda_c                Anne with an E   \n",
       "3  1101617456945287169  StarWarsSession    Star Wars Sessions Podcast   \n",
       "4               821846           kelake           Clark MacLeod （克拉克）   \n",
       "\n",
       "                          location    join_date  followers  following  likes  \\\n",
       "0                   United Kingdom  14 Feb 2020        418        685   1089   \n",
       "1                Sydney, Australia  12 Nov 2012        202        184    264   \n",
       "2                     South Africa   9 Dec 2011        880        838   3881   \n",
       "3                        Essex, UK   1 Mar 2019       1288       1607  16529   \n",
       "4  Stratford, Prince Edward Island   8 Mar 2007        674        558   7373   \n",
       "\n",
       "                                       url  verified  \\\n",
       "0       https://www.bradleystokeradio.com/         0   \n",
       "1       https://www.knowledgebroker.online         0   \n",
       "2  https://linktr.ee/RootofSciencePodcasts         0   \n",
       "3      http://patreon.com/starwarssessions         0   \n",
       "4                  http://clarkmacleod.com         0   \n",
       "\n",
       "                                             geotext                    city  \\\n",
       "0  <geotext.geotext.GeoText object at 0x7f43b453a...                           \n",
       "1  <geotext.geotext.GeoText object at 0x7f43b453a...                  Sydney   \n",
       "2  <geotext.geotext.GeoText object at 0x7f43b453a...                           \n",
       "3  <geotext.geotext.GeoText object at 0x7f43b453a...                   Essex   \n",
       "4  <geotext.geotext.GeoText object at 0x7f43b453a...  StratfordPrince Edward   \n",
       "\n",
       "          country iso3  \n",
       "0  United Kingdom  GBR  \n",
       "1       Australia  AUS  \n",
       "2    South Africa  ZAF  \n",
       "3  United Kingdom  GBR  \n",
       "4          Canada  CAN  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_users_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge User's Geographic Data with Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.merge(left=df_tweets, \n",
    "                   right=df_users_full, \n",
    "                   how='left',\n",
    "                   on='username')\n",
    "\n",
    "full_df.to_csv('data/tweets_users_september.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
